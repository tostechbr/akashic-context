# ğŸ§  Memory Context Engine

> Universal Memory & Context Management for LLMs

**Status**: âœ… **Phase 1 Complete** - Core memory system ready for production (97.7% test coverage)

## Overview

Memory Context Engine is a standalone library that adds intelligent memory and context management to any LLM-based application. Extracted and adapted from the battle-tested [Moltbot](https://github.com/moltbot/moltbot) project.

### Features

- ğŸ§  **Long-term Memory**: Hybrid search (Vector + BM25) for semantic + keyword matching
- ğŸ”Œ **MCP Protocol**: Model Context Protocol server for AI agent integration
- ğŸ¤– **Universal Integration**: Works with Claude Desktop, Cursor, n8n, LangChain, and more
- ğŸ“ **Markdown Storage**: Human-readable, version-control friendly
- ğŸš€ **Battle-Tested**: Code extracted from production Moltbot system
- ğŸ”’ **Local-First**: All data stays on your machine
- ğŸ” **Secure**: Path traversal protection, file size limits, environment-based secrets

## Packages

| Package | Description | Status |
|---------|-------------|--------|
| **[@memory-context-engine/core](./packages/core)** | Core memory system (hybrid search, embeddings, storage) | âœ… **Ready** (86/88 tests) |
| **[@memory-context-engine/mcp-server](./packages/mcp-server)** | MCP Server adapter for AI agents | âœ… **Ready** |

## Quick Start

### Option 1: Use as MCP Server (Recommended)

Perfect for Claude Desktop, Cursor, n8n, and other AI agents.

**1. Install:**
```bash
npm install -g @memory-context-engine/mcp-server
```

**2. Configure (Claude Desktop example):**
```json
// ~/Library/Application Support/Claude/claude_desktop_config.json
{
  "mcpServers": {
    "memory": {
      "command": "memory-mcp-server",
      "env": {
        "MEMORY_WORKSPACE_DIR": "/path/to/your/workspace",
        "OPENAI_API_KEY": "sk-..."
      }
    }
  }
}
```

**3. Use:**
Ask Claude: *"Search my memory for architecture decisions"*

See [MCP Server README](./packages/mcp-server/README.md) for full documentation.

---

### Option 2: Use as Library

Perfect for embedding directly in your TypeScript/JavaScript application.

**1. Install:**
```bash
npm install @memory-context-engine/core
```

**2. Use:**
```typescript
import { MemoryManager } from "@memory-context-engine/core";

const manager = new MemoryManager({
  dataDir: "./data",
  userId: "user-123",
  workspaceDir: "./workspace",
  memory: {
    enabled: true,
    provider: "openai",
    model: "text-embedding-3-small",
    chunkSize: 400,
    chunkOverlap: 80,
  },
});

// Index memory files
await manager.sync();

// Search
const results = await manager.search("database architecture", {
  maxResults: 5,
  minScore: 0.4,
});

console.log(results);
```

See [Core README](./packages/core/README.md) for full API documentation.

## Architecture

### Core + Adapters Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Core Library                          â”‚
â”‚  (@memory-context-engine/core)                          â”‚
â”‚                                                         â”‚
â”‚  â€¢ MemoryManager - Orchestration                       â”‚
â”‚  â€¢ HybridSearch - Vector + Keyword                     â”‚
â”‚  â€¢ MemoryStorage - SQLite persistence                  â”‚
â”‚  â€¢ EmbeddingProviders - OpenAI, Gemini, Local          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†‘                           â†‘
            â”‚                           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  MCP Server   â”‚           â”‚  HTTP API     â”‚
    â”‚  (stdio)      â”‚           â”‚  (future)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†‘                           â†‘
            â”‚                           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Claude       â”‚           â”‚  n8n Cloud    â”‚
    â”‚  Cursor       â”‚           â”‚  Custom Apps  â”‚
    â”‚  n8n (local)  â”‚           â”‚               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- ğŸ¯ **Flexible**: Choose how to integrate (MCP, library, HTTP)
- ğŸ”§ **Maintainable**: Core logic separated from integration details
- ğŸŒ **Open Source Friendly**: Easy to internalize or customize
- ğŸ“¦ **Modular**: Adapters evolve independently

## How It Works

### Memory System (Phase 1 - Complete)

1. **Markdown Files**: Store memories in `MEMORY.md` and `memory/*.md`
2. **Chunking**: Splits content into ~400 token chunks with 80 token overlap
3. **Embeddings**: Generates vector embeddings via OpenAI/Gemini/Local
4. **Storage**: SQLite database with FTS5 (keyword) + sqlite-vec (vector)
5. **Hybrid Search**: Combines vector similarity (70%) + BM25 keyword (30%)
6. **Retrieval**: Returns ranked results with snippets and scores

### Example Memory File

```markdown
# memory/2025-01-project-decisions.md

## Database Choice

We decided to use PostgreSQL for the main database because:
- Strong ACID guarantees
- JSON support for flexible schemas
- Mature ecosystem with great tooling
- Good performance for our scale
```

### Searching

```bash
# Via MCP (Claude Desktop)
"Search memory for database decisions"

# Via Library
const results = await manager.search("database decisions");
// Returns: [{ path, startLine, endLine, score, snippet }]
```

## Development Status

### âœ… Phase 1: Memory System (Complete - 97.7%)

**Core Library** (`@memory-context-engine/core`):
- âœ… Chunking algorithm (21/21 tests passing)
- âœ… Hybrid search (25/25 tests passing)
- âœ… SQLite storage (20/20 tests passing)
- âœ… Memory Manager (18/20 tests passing - 2 edge cases)
- âœ… OpenAI embedding provider
- âœ… Utilities (hash, tokens, files)
- âœ… **Total: 86/88 tests passing** ğŸ‰

**MCP Server** (`@memory-context-engine/mcp-server`):
- âœ… MCP Protocol implementation
- âœ… Tools: `memory_search`, `memory_get`
- âœ… stdio transport
- âœ… Path traversal protection
- âœ… File size limits (10MB)
- âœ… Full documentation + examples
- âœ… Build passing

### ğŸš§ Phase 2: Context Management (Planned)

- [ ] Session management
- [ ] Token counting
- [ ] Message compaction
- [ ] Context pruning

### ğŸ“‹ Phase 3: Additional Adapters (Future)

- [ ] HTTP API server (for cloud services)
- [ ] LangChain Python integration
- [ ] LangChain.js integration

## Testing

```bash
# Install dependencies
pnpm install

# Build native modules (better-sqlite3)
cd node_modules/.pnpm/better-sqlite3*/node_modules/better-sqlite3
npm run build-release

# Run tests
pnpm test

# Build all packages
pnpm build
```

### Test MCP Server Locally

```bash
cd packages/mcp-server
node test-simple.js
```

See [CLAUDE.md](./CLAUDE.md) for detailed setup instructions.

## Security

- âœ… **Path Traversal Protection**: Prevents reading files outside workspace
- âœ… **File Size Limits**: 10MB max to prevent OOM attacks
- âœ… **Environment Variables**: API keys never hardcoded
- âœ… **Input Validation**: Zod schemas for all inputs

See [packages/mcp-server/SECURITY.md](./packages/mcp-server/SECURITY.md) for details.

## Documentation

- **[CLAUDE.md](./CLAUDE.md)**: Development guide, setup, troubleshooting
- **[Core Package](./packages/core/README.md)**: Core library API
- **[MCP Server](./packages/mcp-server/README.md)**: MCP Server usage
- **[Examples](./examples/README.md)**: Usage examples

## Contributing

This is an open-source project. Contributions welcome!

1. Fork the repository
2. Create your feature branch
3. Add tests
4. Submit a pull request

## Credits

This project extracts and adapts core memory and context management systems from [Moltbot](https://github.com/moltbot/moltbot), an open-source personal AI assistant.

## License

MIT License - See [LICENSE](./LICENSE) for details

## Author

Tiago Santos ([@tostechbr](https://github.com/tostechbr))

---

**Built with** â¤ï¸ **by developers who believe AI should remember your conversations**
