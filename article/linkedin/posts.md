Quanto custa rodar +4.900 an√°lises com um Agente OCR?

Recentemente dei manuten√ß√£o em um agente OCR e precisei criar uma f√≥rmula para auditar cada centavo gasto com o Gemini 2.5 Pro. Levei os dados para o Excel e mapeei Input, Output e, principalmente, o Thinking.

A f√≥rmula que valida o custo real:
Custo = (Input x $1.25) + ((Output + Thinking) x $10.00) / 1M

Desde o dia 27/11/2025 at√© hoje, o agente processou exatos 4.909 documentos complexos.

N√£o foi uma leitura simples. Usei a t√©cnica do Chain of Thought (CoT) no prompt, for√ßando o modelo a raciocinar antes de extrair os dados. Isso garante precis√£o, mas tem um "pre√ßo invis√≠vel".

Os N√∫meros da Opera√ß√£o:
‚Üí Total de Docs: 4.909
‚Üí Tokens de Input: 4.0 Milh√µes
‚Üí Tokens de Output (Texto): 880 Mil
‚Üí Tokens de Thinking (Racioc√≠nio): 4.25 Milh√µes ü§Ø

O modelo gerou 4x mais pensamento do que texto final. Isso significa que mais de 80% do custo de sa√≠da foi o Agente "refletindo" antes de responder. √â o custo invis√≠vel da intelig√™ncia.

A Conta Final:
‚Üí Custo M√©dio por Doc: $0.011
‚Üí Custo Total da Opera√ß√£o: $56.38 (aprox. R$ 302,00)

Conclus√£o:
Por 6 centavos de real, um "analista" lendo, pensando e estruturando quase 5 mil documentos. A precis√£o do CoT pode pagar cada centavo desse processamento.

---

Em 2026, o RAG que usamos hoje ser√° considerado "burro".

Essa leitura de 10 minutos do artigo da Leonie Monigatti pode economizar horas de refatora√ß√£o na arquitetura de novos agentes.

Como estou desenvolvendo um projeto open source, tenho estudado novas arquiteturas de IA. E, ao analisar os diagramas abaixo, a progress√£o fica n√≠tida, quando o agente deixa de ser s√≥ leitor e passa a ser leitor + escritor.

√â isso que Leonie chama de expans√£o do fluxo: n√£o √© sobre ‚Äúmais features‚Äù, √© sobre como a informa√ß√£o flui.

‚Üí RAG Tradicional: read-only, one-shot.
‚Üí Agentic RAG: leitura mais inteligente.
‚Üí Agent Memory: read-write. O sistema aprende e personaliza.

Na pratica significa sair de um "Chatbot de Suporte" para um "Consultor Pessoal".

Imagine um m√©dico perguntando no OpenEvidence:

‚ÄúQual o tratamento recomendado para pneumonia adquirida na comunidade?‚Äù

‚Üí No RAG (indexa√ß√£o est√°tica):

O sistema busca nos artigos indexados e devolve alguns trechos (3 chunks, por exemplo) de estudos e guidelines. O m√©dico precisa ler tudo e extrair manualmente o que importa.

‚Üí No Agent Memory:

1. O agente busca os artigos relevantes na base (mesmo passo do RAG).
2. Antes de responder, ele consulta a Mem√≥ria.
3. Ele encontra um registro: ‚ÄúUsu√°rio prefere respostas em bullets, com dosagem e contraindica√ß√µes destacadas.‚Äù
4. A√ß√£o: O agente organiza automaticamente a evid√™ncia em uma tabela de recomenda√ß√£o + resumo executivo.

‚Üí Resultado: O m√©dico recebe uma s√≠ntese clara, com bullets, tabela de tratamento e cita√ß√µes, sem ter pedido esse formato dessa vez.

A virada de chave da arquitetura n√£o √© apenas conectar no banco de dados da empresa, √© criar um sistema personalizado ao longo do tempo que evolui com o uso. O RAG traz o dado, a Mem√≥ria traz o contexto.

Claro, gerenciar essa mem√≥ria (o que esquecer? o que √© fato vs opini√£o?) √© o novo desafio da engenharia. Mas estou atacando/aprendendo como lidar.

Link do artigo nos coment√°rios üëá

---

Todo Engenheiro de IA precisa saber disso!

O problema n√£o √© o tamanho do seu prompt. 

Recentemente, mergulhei no ebook de Context Engineering do Weaviate e tive alguns insights que mudam o jogo na constru√ß√£o de Agentes.

Frequentemente recebo pedidos de ajuda em projetos onde a preocupa√ß√£o principal √©: "Meu prompt est√° grande demais, o modelo vai se perder?". Mas a verdade dura √© que o problema raramente √© o tamanho, e sim a falta de t√©cnica e estrutura.

N√£o basta escrever um texto longo. √â preciso saber diferenciar Prompt Engineering (como voc√™ pede) de Context Engineering (o que voc√™ entrega para a IA saber).

Estou aplicando a estrat√©gia avan√ßada de ReAct Prompting (Reason + Act) em um novo Agente de Voice AI focado em cobran√ßa e a diferen√ßa √© brutal.

Em vez de o modelo tentar "alucinar" ou dar uma resposta gen√©rica quando o cliente pede pare renegociar, o ReAct for√ßa a IA a entrar num ciclo de racioc√≠nio l√≥gico antes de falar:

> Pensar: "O cliente confirmou a pend√™ncia, mas disse que est√° apertado. Preciso formular uma proposta vi√°vel."
> Agir: "Vou consultar as regras de negocia√ß√£o do workflow que recebeu no prompt ou chamar a tool calcular_divida."
> Observar: "O sistema permite parcelar e quitar √† vista."
> Responder: "Entendo seu cen√°rio. Consigo fazer uma condi√ß√£o especial em 3 parcelas... Fica melhor para voc√™?"

Isso transforma um chatbot que apenas "cobra" em um Agente que entende o contexto, consulta dados din√¢micos e negocia em tempo real.

Se voc√™ ainda est√° preso apenas no "Zero-shot" (perguntar e esperar a resposta), est√° deixando muito potencial na mesa.

Voc√™s j√° aplicam frameworks como ReAct ou Tree of Thoughts (ToT) nos fluxos de voc√™s? üëá

Coloquei o link do ebook nos coment√°rios

---

O agente errava mais da metade das decis√µes. Agora erra 1 em cada 7.

44% ‚Üí 86% de acur√°cia em 5 vers√µes de prompt.

O case √©: um agente de WhatsApp que decide se resolve o problema do usu√°rio sozinho ou chama um humano.

Ach√°vamos que estava funcionando. Mas n√£o tinha como comprovar. 

A solu√ß√£o come√ßou com observabilidade.

Fiz o projeto com o Lucas Rodrigues, que tem experi√™ncia em Evals e Machine Learning. Decidimos usar o Langfuse para rastrear cada decis√£o do agente. Input, output, racioc√≠nio e outros dados relevantes. 

Depois de algumas reuni√µes dele me passando conhecimento e discutindo sobre a base de Evals e ML, decidimos aplicar o seguinte:

‚Üí Selecionar 50 traces reais no Langfuse
‚Üí Criar um dataset com esses traces
‚Üí Fazer human annotation: "deveria escalar?" sim ou n√£o, e por qu√™
‚Üí Rodar experiments (novos prompts) em cima do dataset
‚Üí Comparar resultados com as anota√ß√µes humanas

Essa t√©cnica se chama evaluation com human annotation. √â a base de qualquer melhoria de agente. E foi baseada no texto "LLM Evals: Everything You Need to Know" de Hamel Husain.

O resultado? 44% de acur√°cia. O agente escalava quase tudo. Chamava humano pra resolver coisas que ele mesmo poderia fazer.

Depois de 5 vers√µes refinando o prompt: 86% de acur√°cia. Hoje j√° estamos na vers√£o 9, focados em outras melhorias.

Acur√°cia = (acertos / total de casos)

√â a m√©trica mais simples de ML. E a mais ignorada em agentes de IA.

Voc√™ n√£o pode colocar o agente em produ√ß√£o e torcer pra dar certo. Observabilidade + dataset + anota√ß√£o humana + m√©tricas √© o que separa achismo de engenharia.

---

Quanto custa rodar +4.900 an√°lises com um Agente OCR?

Recentemente dei manuten√ß√£o em um agente OCR e precisei criar uma f√≥rmula para auditar cada centavo gasto com o Gemini 2.5 Pro. Levei os dados para o Excel e mapeei Input, Output e, principalmente, o Thinking.

A f√≥rmula que valida o custo real:
Custo = (Input x $1.25) + ((Output + Thinking) x $10.00) / 1M

Desde o dia 27/11/2025 at√© hoje, o agente processou exatos 4.909 documentos complexos.

N√£o foi uma leitura simples. Usei a t√©cnica do Chain of Thought (CoT) no prompt, for√ßando o modelo a raciocinar antes de extrair os dados. Isso garante precis√£o, mas tem um "pre√ßo invis√≠vel".

Os N√∫meros da Opera√ß√£o:
‚Üí Total de Docs: 4.909
‚Üí Tokens de Input: 4.0 Milh√µes
‚Üí Tokens de Output (Texto): 880 Mil
‚Üí Tokens de Thinking (Racioc√≠nio): 4.25 Milh√µes ü§Ø

O modelo gerou 4x mais pensamento do que texto final. Isso significa que mais de 80% do custo de sa√≠da foi o Agente "refletindo" antes de responder. √â o custo invis√≠vel da intelig√™ncia.

A Conta Final:
‚Üí Custo M√©dio por Doc: $0.011
‚Üí Custo Total da Opera√ß√£o: $56.38 (aprox. R$ 302,00)

Conclus√£o:
Por 6 centavos de real, um "analista" lendo, pensando e estruturando quase 5 mil documentos. A precis√£o do CoT pode pagar cada centavo desse processamento.

